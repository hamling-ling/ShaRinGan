{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#import_tf_python\n",
    "https://qiita.com/dcm_sakai/items/0e13e2917adf55e92745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import uff\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_file_in   = '../data/output/frozen_model/frozen.pb'\n",
    "uff_file_out = '../data/output/frozen_model/frozen.uff'\n",
    "uff.from_tensorflow_frozen_model(frozen_file=pb_file_in,\n",
    "                                 output_nodes=[\"generator/Tanh\"],\n",
    "                                 output_filename=uff_file_out,\n",
    "                                 list_nodes=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"../data/input/evaluation/002080.bin\"\n",
    "\n",
    "raw_data=np.fromfile(fn, np.float32)\n",
    "print(\"raw data shape=\", raw_data.shape)\n",
    "\n",
    "raw_data_reshaped = np.reshape(raw_data,[2,1,-1,1])\n",
    "\n",
    "input  = raw_data_reshaped[0].reshape([1,1,-1,1])\n",
    "target = raw_data_reshaped[1].reshape([1,1,-1,1])\n",
    "print(input.shape)\n",
    "plt.plot(input[0,0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "    parser.register_input(\"input\", (1, 1, 1024))\n",
    "    parser.register_output(\"generator/Tanh\")\n",
    "    parser.parse(\"../data/output/frozen_model/frozen.uff\", network)\n",
    "    \n",
    "    builder.max_workspace_size = 1 <<  20\n",
    "    builder.fp16_mode = True\n",
    "    builder.strict_type_constraints = True\n",
    "    with builder.build_cuda_engine(network) as engine:\n",
    "        # Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.\n",
    "        h_input  = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=np.float32)\n",
    "        h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=np.float32)\n",
    "        # Allocate device memory for inputs and outputs.\n",
    "        d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "        d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "        # Create a stream in which to copy inputs/outputs and run inference.\n",
    "        stream = cuda.Stream()\n",
    "        \n",
    "        with engine.create_execution_context() as context:\n",
    "            np.copyto(h_input, input[0,0,:,0])\n",
    "            # Transfer input data to the GPU.\n",
    "            cuda.memcpy_htod_async(d_input, h_input, stream)\n",
    "            # Run inference.\n",
    "            context.execute_async(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)\n",
    "            # Transfer predictions back from the GPU.\n",
    "            cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "            # Synchronize the stream\n",
    "            stream.synchronize()\n",
    "            # Return the host output. \n",
    "            plt.plot(h_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.gfile.GFile(\"../data/output/frozen_model/frozen.pb\", \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with tf.Graph().as_default() as gf:\n",
    "    tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    graph = gf\n",
    "\n",
    "X=graph.get_tensor_by_name('prefix/input:0')\n",
    "output_node = graph.get_tensor_by_name('prefix/generator/Tanh:0')\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    frozen_output=output_node.eval({X: input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(1024)\n",
    "plt.plot(x, input[0,0,:,0])\n",
    "plt.plot(x, frozen_output[0,0,:,0], '-')\n",
    "plt.plot(h_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
