{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://docs.nvidia.com/deeplearning/sdk/tensorrt-developer-guide/index.html#import_tf_python\n",
    "https://qiita.com/dcm_sakai/items/0e13e2917adf55e92745"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorrt as trt\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import uff\n",
    "import pycuda.driver as cuda\n",
    "import pycuda.autoinit\n",
    "import os\n",
    "import struct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!./sharingan_export.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pb_file_in   = '../data/output/frozen_model/frozen.pb'\n",
    "uff_file_out = '../data/output/frozen_model/frozen.uff'\n",
    "uff.from_tensorflow_frozen_model(frozen_file=pb_file_in,\n",
    "                                 output_nodes=[\"generator/Tanh\"],\n",
    "                                 output_filename=uff_file_out,\n",
    "                                 list_nodes=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fn = \"../data/input/temp/000599.bin\"\n",
    "\n",
    "raw_data=np.fromfile(fn, np.float32)\n",
    "print(\"raw data shape=\", raw_data.shape)\n",
    "\n",
    "raw_data_reshaped = np.reshape(raw_data,[2,1,-1,1])\n",
    "\n",
    "input  = raw_data_reshaped[0].reshape([1,1,-1,1])\n",
    "target = raw_data_reshaped[1].reshape([1,1,-1,1])\n",
    "print(input.shape)\n",
    "plt.plot(input[0,0,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SharinganCalibrator(trt.IInt8EntropyCalibrator2):\n",
    "    def __init__(self, batch_data_dir, cache_file):\n",
    "        print(\"in. Calibrator init\")\n",
    "        # Whenever you specify a custom constructor for a TensorRT class,\n",
    "        # you MUST call the constructor of the parent explicitly.\n",
    "        trt.IInt8EntropyCalibrator2.__init__(self)\n",
    "\n",
    "        self.cache_file = cache_file\n",
    "        # Get a list of all the batch files in the batch folder.\n",
    "        self.batch_files = [os.path.join(batch_data_dir, f) for f in os.listdir(batch_data_dir)]\n",
    "        # Find out the shape of a batch and then allocate a device buffer of that size.\n",
    "        #self.shape, _, _ = self.read_batch_file(self.batch_files[0])\n",
    "        self.shape,_ = self.read_batch_file(self.batch_files[0])\n",
    "        # Each element of the calibration data is a float32.\n",
    "        self.device_input = cuda.mem_alloc(trt.volume(self.shape) * trt.float32.itemsize)\n",
    "\n",
    "        # Create a generator that will give us batches. We can use next() to iterate over the result.\n",
    "        def load_batches():\n",
    "            for f in self.batch_files:\n",
    "                #shape, data, labels = self.read_batch_file(f)\n",
    "                shape, data = self.read_batch_file(f)\n",
    "                #yield shape, data, labels\n",
    "                yield shape, data\n",
    "        self.batches = load_batches()\n",
    "        print(\"out. Calibrator init\")\n",
    "\n",
    "    # This function is used to load calibration data from the calibration batch files.\n",
    "    # In this implementation, one file corresponds to one batch, but it is also possible to use\n",
    "    # aggregate data from multiple files, or use only data from portions of a file.\n",
    "    def read_batch_file(self, filename):\n",
    "        #with open(filename, \"rb\") as f:\n",
    "            # Read the first 4 integers. These will be the NCHW dimensions of the data.\n",
    "            #shape = tuple(struct.unpack(\"<L\", f.read(trt.int32.itemsize))[0] for _ in range(4))\n",
    "            # Next read in all the images, where each element of each image is a float32.\n",
    "            # The remainder of the file consists of labels\n",
    "            #labels = f.read()\n",
    "        #return shape, data, labels\n",
    "        raw_data=np.fromfile(filename, np.float32)\n",
    "        #print(\"raw data shape=\", raw_data.shape)\n",
    "        raw_data_reshaped = np.reshape(raw_data,[2,1,-1,1])\n",
    "        #input  = raw_data_reshaped[0].reshape([1,1,-1,1])\n",
    "        #print(input)\n",
    "        #return (1,1,1024,1), input[0].tobytes()\n",
    "        input  = raw_data_reshaped[0].reshape([1,1,-1])\n",
    "        return (1,1,1024,), input.tobytes()\n",
    "\n",
    "    def get_batch_size(self):\n",
    "        #print(\"in. Calibrator get_batch_size\")\n",
    "        #print(\"out. Calibrator get_batch_size\", self.shape[0])\n",
    "        return self.shape[0]\n",
    "\n",
    "    # TensorRT passes along the names of the engine bindings to the get_batch function.\n",
    "    # You don't necessarily have to use them, but they can be useful to understand the order of\n",
    "    # the inputs. The bindings list is expected to have the same ordering as 'names'.\n",
    "    def get_batch(self, names):\n",
    "        try:\n",
    "            # Get a single batch.\n",
    "            #_, data, _ = next(self.batches)\n",
    "            _, data = next(self.batches)\n",
    "            # Copy to device, then return a list containing pointers to input device buffers.\n",
    "            cuda.memcpy_htod(self.device_input, data)\n",
    "            return [int(self.device_input)]\n",
    "        except StopIteration:\n",
    "            # When we're out of batches, we return either [] or None.\n",
    "            # This signals to TensorRT that there is no calibration data remaining.\n",
    "            return None\n",
    "\n",
    "    def read_calibration_cache(self):\n",
    "        # If there is a cache, use it instead of calibrating again. Otherwise, implicitly return None.\n",
    "        if os.path.exists(self.cache_file):\n",
    "            with open(self.cache_file, \"rb\") as f:\n",
    "                return f.read()\n",
    "\n",
    "    def write_calibration_cache(self, cache):\n",
    "        with open(self.cache_file, \"wb\") as f:\n",
    "            f.write(cache)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "TRT_LOGGER = trt.Logger(trt.Logger.WARNING)\n",
    "with trt.Builder(TRT_LOGGER) as builder, builder.create_network() as network, trt.UffParser() as parser:\n",
    "    \n",
    "    batch_data_dir =  \"../data/input/temp\"\n",
    "    calibration_cache = \"sharingan_calibration.cache\"\n",
    "    calib = SharinganCalibrator(batch_data_dir, cache_file=calibration_cache)\n",
    "    \n",
    "    parser.register_input(\"input\", (1, 1, 1024))\n",
    "    parser.register_output(\"generator/Tanh\")\n",
    "    parser.parse(\"../data/output/frozen_model/frozen.uff\", network)\n",
    "    \n",
    "    builder.max_batch_size = calib.get_batch_size()\n",
    "    builder.int8_mode = True\n",
    "    builder.int8_calibrator = calib\n",
    "    #builder.fp16_mode = True\n",
    "    \n",
    "    input_tensor = network.get_input(0)\n",
    "    input_tensor.set_dynamic_range(-1.0, 1.0)\n",
    "    output_tensor = network.get_output(0)\n",
    "    output_tensor.set_dynamic_range(-1.0, 1.0)\n",
    "    \n",
    "    with builder.build_cuda_engine(network) as engine:\n",
    "        for binding in engine:\n",
    "            dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "            print(dtype)\n",
    "        # Determine dimensions and create page-locked memory buffers (i.e. won't be swapped to disk) to hold host inputs/outputs.\n",
    "        h_input  = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(0)), dtype=np.float32)\n",
    "        h_output = cuda.pagelocked_empty(trt.volume(engine.get_binding_shape(1)), dtype=np.float32)\n",
    "        # Allocate device memory for inputs and outputs.\n",
    "        d_input = cuda.mem_alloc(h_input.nbytes)\n",
    "        d_output = cuda.mem_alloc(h_output.nbytes)\n",
    "        # Create a stream in which to copy inputs/outputs and run inference.\n",
    "        stream = cuda.Stream()\n",
    "        \n",
    "        with engine.create_execution_context() as context:\n",
    "            #input_u8 = 255.0 * (input + 1.0)/2.0\n",
    "            #input_u8 = input_u8.astype(np.uint8)\n",
    "            np.copyto(h_input, input[0,0,:,0])\n",
    "            # Transfer input data to the GPU.\n",
    "            cuda.memcpy_htod_async(d_input, h_input, stream)\n",
    "            # Run inference.\n",
    "            context.execute_async(bindings=[int(d_input), int(d_output)], stream_handle=stream.handle)\n",
    "            # Transfer predictions back from the GPU.\n",
    "            cuda.memcpy_dtoh_async(h_output, d_output, stream)\n",
    "            # Synchronize the stream\n",
    "            stream.synchronize()\n",
    "            # Return the host output. \n",
    "            plt.plot(h_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for binding in engine:\n",
    "    dtype = trt.nptype(engine.get_binding_dtype(binding))\n",
    "    print(dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.gfile.GFile(\"../data/output/frozen_model/frozen.pb\", \"rb\") as f:\n",
    "    graph_def = tf.GraphDef()\n",
    "    graph_def.ParseFromString(f.read())\n",
    "\n",
    "with tf.Graph().as_default() as gf:\n",
    "    tf.import_graph_def(graph_def, name=\"prefix\")\n",
    "    graph = gf\n",
    "\n",
    "X=graph.get_tensor_by_name('prefix/input:0')\n",
    "output_node = graph.get_tensor_by_name('prefix/generator/Tanh:0')\n",
    "with tf.Session(graph=graph) as sess:\n",
    "    frozen_output=output_node.eval({X: input})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x=np.arange(1024)\n",
    "plt.plot(x, input[0,0,:,0])\n",
    "plt.plot(x, frozen_output[0,0,:,0], '-')\n",
    "plt.plot(h_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
