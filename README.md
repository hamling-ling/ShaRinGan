# ShaRinGan

Deep learning model using DCGAN to converts raw guitar sound into effector modified sound.

## Prerequisites

- NVIDIA GPU (tested with GTX1060)
- Linux (tested with Ubuntu 20.04 LTS)
- Docker and NVIDIA Container Toolkit (see [Installation Guide](https://docs.nvidia.com/datacenter/cloud-native/container-toolkit/install-guide.html#docker))

## Jetson Nano Application

Guitar Effector application using this model can be found in src/app directory.<br>Go to ["Jetson Nano DCGAN Guitar Effector"]( ./src/app/README.md "Jetson Nano DCGAN Guitar Effector") for details.

## Getting Started

You need to have /home/YOU/Github directory.
```
cd ~
mkdir Github
```

Clone this repository
```
cd ~/Github
git clone https://github.com/hamling-ling/ShaRinGan.git
```

Build docker image
```
cd ShaRinGan/docker
docker build -f Dockerfile \
	--build-arg user=$(id -un) \
	--build-arg user_id="$(id -u)" \
    --build-arg user_grp="$(id -gn)" \
	--build-arg user_gid="$(id -g)" \
	--build-arg pass=1234 \
	-t tensor-plus .

```

Run docker image
```
./tf.sh
```

In the docker container, download training data set. This takes about 30 sec.
```
cd Github/ShaRinGan/data/raw_waves
./download.sh
```

Convert wave files to bin files
```
cd ../../src
python create_waves_wavfiles.py
```

Run Training(you can skip)
```
# this takes 1-2 days with GPU enabled machine
python sharingan_train.sh
```

Or Download pre-trained model
```
cd ../../data/output
download_pretrained.sh
```

Run Inference to convert raw to effectored suond
```
$cd src
$./sharingan_cvt.sh
```

You will see raw input, ground truth and converted wav files
```
$ls ../data/cvt/pretrained_model/
input.wav  output.wav  target.wav
```

- [input.wav]( https://soundcloud.com/osakana-zabuun/raw-guitar-sound-as-neural-network-input ) as an input source.
- [target.wav]( https://soundcloud.com/osakana-zabuun/ground-truth-should-for-a-neural-network-model ) is a ground truth.
- [output.wav]( https://soundcloud.com/osakana-zabuun/output ) is what generated by GCGAN.

## Visualize

Run visualizing script
```
$cd src
./sharingan_test.sh
```

Then you'll get bunch of images like this.
Note that blue: input source, green: ground truth and orange: inference.

![visualized](https://i.imgur.com/333MiJB.png "visualized")

## Exporting to TensorRT

You need to work in another docker image.
```
# build docker image for TensorRT
cd ShaRinGan/docker
docker build -f Dockerfile_trt \
	--build-arg user=$(id -un) \
	--build-arg user_id="$(id -u)" \
   --build-arg user_grp="$(id -gn)" \
	--build-arg user_gid="$(id -g)" \
	--build-arg pass=1234 \
	-t tensor-rt .
```
Run the container
```
./trt.sh
```

In the above container, export tensorflow model as a frozen model.
This will create a data/output/frozen_model directory.
```
cd Github/ShaRinGan/src
sharingan_export.sh
```

Then convert to TensorRT model.
This command will generated uff and tensor rt engine file in data/output/frozen_model directory.
```
python3 tensorrt_export_fp.py
```

## License

This project is licensed under the MIT License - see the [LICENSE.md](LICENSE.md) file for details


